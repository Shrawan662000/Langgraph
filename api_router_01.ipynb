{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANYAAAD5CAIAAADUe1yaAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XlcE2f+B/BnchFycYSbcFM5BEHFowKiRet61mO1q1hrq7ulVbttrXb7q7u129Zt7bF2a11rbT273icuivUWFPFCi4hCCEcIQRIgIfc1vz/ii7IYDjUzz4Q871f/gCGZ50v98GTmmWeewXAcBwgCDw12AYi7QxFEIEMRRCBDEUQgQxFEIEMRRCBjwC7gSahbzGqlWae2atstFpNrDCsxmBidgXH4dI6AIQxmsTl02BVRBeYa/4AAAACaGwziW1rJHS1XwLBacI6AzuUzWJ404Aq/AcMD07RadO1WndqiVVm5XvSoJO4zqTyeDxN2aZC5RgRVSvOlowo6E/MJYEUN5PqFesCu6Gk1iPWSMm2L3Ojtzxo1Vchguu8RkQtE8Mpx5b1r7aOm+cWm8GDX4ny3LrRdylNmzvBLGuUFuxY4qB7B/d9Ik9IF8WkC2IUQq6Sgpb3FnD03EHYhEFA3gjiOb3q/etprIcFRnrBrIUP5FXXNHe2kV4NhF0I26kbw3yurFqyK5Apc8pz9yVRcVZddUv/+zyLYhZCKohHcv06aPl0YHOkW/V9nvxaplDLjmNkBsAshDxVPxIrzlcmZAjfMHwAgOd2Lw6ffLVHDLoQ8lItg6wNTVakmbmg/P//owZBsn3P7mmFXQR7KRfBSnnLUVCHsKmBiMGlDx/lcOa6EXQhJqBVBeY3Bw5MWndwPx/8ey/AJvvIag9lkg10IGagVQfFtjW8Qi7TmysrKjEYjrLf3jM2lS8q0BO2cUqgVQckdbdRALjlt5eXlLVy4UK/XQ3l7r6KSuCiCZGt9YBL4MnwCSeoFn7gDsw9jEdf/2UUnc1VKM6FNUASFIqhSmDEMI2LPtbW1ubm5GRkZkyZNWrNmjc1my8vL++yzzwAA48aNS0tLy8vLAwCUlpYuXbo0IyMjIyPjtddeu3v3rv3tbW1taWlpO3bsWLVqVUZGxh//+EeHb3cuBpOmabNoVRan75lqKHTtQae2cgSEzKL7+OOPa2pqli9frtVqr127RqPR0tPT58+fv3PnznXr1vF4vPDwcACATCYzGo2LFy+m0Wj79u1788038/Ly2Gy2fSc//vjj7NmzN27cSKfTAwMDH32703EFDK3awvWi0L8RESj062nVFoIux8lksvj4+BkzZgAA5s+fDwDw9fUViUQAgKSkJG9vb/vLJk6cOGnSJPvXiYmJubm5paWlI0eOtG9JTk5esmRJxz4ffbvTcb3oWpUVhBG0e6qgUAQBwBkehHwQT5o0aevWrWvXrl28eLGvr293L8Mw7OzZszt37pRIJBwOBwCgVP42ODd8+HAiauuBB5uO26h4+dS5KHQs6MlltLcQcuizZMmSd9555+TJk9OmTdu7d293L9u8efOKFSsSExO//vrrt956CwBgs/02MufpSfYFwzaFieMGszQoFEGOgK5TW4nYM4Zh8+bNO3LkSFZW1tq1a0tLSzt+1DFLw2g0btmyZfr06cuXL09NTU1OTu7Lngmd5EHcwTGlUCiCfF8mk5gPYvsACpfLzc3NBQBUVFR09GrNzQ+vxur1eqPRmJCQYP+2ra2tSy/YRZe3E4Hvy+B79/9ekEK/oX+oR0OVXtNm4Tn7//t7773H4/FGjhxZWFgIALDnLCUlhU6nf/nll9OmTTMajbNmzYqNjd29e7dQKNRoNJs2baLRaFVVVd3t89G3O7fmmnItk0XDaIT8TVIKffXq1bBr+E1bs9lssAWEs527W6lUWlhYeOLECb1ev2zZsjFjxgAABAJBYGDgL7/8cvHiRbVaPWXKlCFDhhQVFe3du7e2tnbZsmUREREHDhzIyckxm83bt2/PyMhITEzs2Oejb3duzTfPtoXGegaEOfl/BQVRa8pqXYW2ukw75vduNGGzO3mbZGPn+PO8+/8tnhT6IAYAhMdzrxxvkdcagiIc//W3tbVNnz7d4Y9EIpFUKn10e1ZW1kcffeTsSrtavHixw0/thISEjqssnQ0dOvSrr77qbm9ll1Q8b4Y75I9yvSAAoKFKf+WEcuZSx/dPWK3WpqYmhz/CMMe/i6enp4+Pj7PL7Kq5udlsdnBJt7uqPDw8hMJup0Vuer/65b9FeHj2/9NhKkYQAHB274NnBvNEz3BgFwLHr0Uqk8E2NJvwPxuKoNCgTIexcwJObJPrNYSMEVJc3T1d9W2N++SPohEEAMxdGf6fz+tgV0G29lbzLzubXng9FHYhpKLiB7GdUW/9+bO6nL+Eu8khUVOt4eTOppz3w2luMBbYGXUjaO8Vdq2tn/ZacFB/v6Hz3nX1rQuqOW/391kxjlA6gnandzXptdb0qX6kTagmk7RSV5SnFMV6pk/zg10LHC4QQQCApExblKeITuYGhrOjkrj94KPKoLVK7mgbJQaVwpw+Vej0C0IuxDUiaFd5s73ypkZSpk0YIWCwMK6AwfWie7DpLvEL0OmYVm3RqS0alUXdYmmqNUQN5A4Yyg+Pc9Oxpw6uFMEONXe1qgdmrdqiVVktFpvNqaM3ZrO5vLw8JSXFmTsFwJNHx204R8DgeTGEwayQmH5+dNt3LhlBQimVyrlz5548eRJ2Ie6CouOCiPtAEUQgQxHsCsOwAQMGwK7CjaAIdoXj+P3792FX4UZQBLvCMMzLy00Xv4cCRbArHMdVKhXsKtwIiqADQUFBsEtwIyiCDsjlctgluBEUwa4wDOt8pxxCNBTBrnAcLy8vh12FG0ERRCBDEewKw7AeVt9CnA5FsCscx1taWmBX4UZQBB3w83PTCcxQoAg6oFAoYJfgRlAEEchQBLvCMCwmJgZ2FW4ERbArHMfFYjHsKtwIiiACGYqgAx3L/SIkQBF0wOGKgAhBUAQRyFAEu0IzZUiGItgVmilDMhRBBDIUwa7QTZwkQxHsCt3ESTIUQQQyFMGu0H3EJEMR7ArdR0wyFMGu0EwZkqEIdoVmypAMRRCBDEXQgcDAQNgluBEUQQe6e9IiQgQUQQfQfEEyoQg6gOYLkglFsCs0WYtkKIJdoclaJEMRdEAkcvxMeIQI6NE3Dy1atEgul9PpdJvN1tra6uvri2GYxWLJz8+HXVo/h3rBh+bMmdPe3i6TyeRyudFobGxslMlkGObyz1ukPhTBhyZMmBAdHd15C47jQ4cOhVeRu0AR/M3cuXM5nN+eixkUFDRv3jyoFbkFFMHfTJgwISIiwv61vQuMj4+HXVT/hyL4PxYsWMDlcu1d4Ny5c2GX4xZQBP/H+PHjIyIicBwfPHgwukxHDgbsApxGq7Io5SaL+WnHmKY//xrQHf7d6Jery7RPuSsPNk0YwmJz6E+5n/6tP4wLtreaz+9vflBvDE/g6dQW2OX8hs7EGip14fGcCQsC0fhOd1w+gpo2y+ENDWNeDPbyY8GuxTFppbb0rPL3b4qYHuiwxwGXj+D6t6sWfBhD8T5G2Wi8nNc0d0U47EKoyLX/Li/nK0e94E/x/AEAhMEeIdGcimtq2IVQkWtHsLHawPeh6OdvF2we40G9EXYVVOTaEbRZcb43E3YVfSIQMo161z7mIYhrR1Crtthg19BHNisw6aywq6Ai144g0g+gCCKQoQgikKEIIpChCCKQoQgikKEIIpChCCKQoQgikKEIIpChCCKQuVcEVaq2sdlp23ds7uN2AIBOp5s1e4LN9tu1aKm07rXc+cQX6y7cK4JPQCKpamlR3rlzu2NL8ZVCSY3YYqHQHQIuDUWwF+LqSgDAxcKzHVuKiwvNZnNNTTXUuvoPFMFeSCRVAICionP2b3U63a3bNwAAlVUVsEvrJ1AEeyGurgwNDZM1NojFlQCAGzdLLBZLaIioshJF0DlQBHshqa4al/270BBRYdE5+6dwQkLS0KEjKqvuwS6tn0AR7ElTk1yj1URGxmRljSssPAsAuFJSNDrzucjIGLH4fufTZOSJoQj2xH4gGB0Vm5U1rkp8v7DwnELRnJn5XHRUrF6vl0rrYBfYH/SfBT2IIK6uZDKZISEiBoMREhy6fsOXMTHPhIaI+HwBAOB+ZUV4eCTsGl0e6gV7IpFUhYVFMBgMAEBW1rimJvnozGwAgIAv8PPzR2ckTuGOveDt2zc6XwiJiooZlDzY4XZxdWVszAD7t1lZ43bt3paZMdb+bXRULIqgU7hjBK/fKLl+o6Tj27Fjxtsj2GV71uhsqbRuXPZE+7dxAxKGD3s2Kurhc2KjomLz8w+TXns/5Nprymz7uGb8AhHf2wX+kOoqtDW/qicvDoZdCOWgY0EEMhRBBDIUQQQyFEEEMhRBBDIUQQQyFEEEMhRBBDIUQQQyFEEEMhRBBDIUQQQyFEEEMteOoDCIBWyuMdMHw0C9vNKl5yURxLUjSGfSlI0G2FX0yYN6gyjCDz3j+FGuHcHoJI5S5hqPNGpvMQ0fE717924AwKlTp2CXQyGuHcGKhrNmo/XWeSXsQnpx8ZA8NIYdEMa2fxsREZGZmWk0usYfD9FceNb0okWLli1blpqaeuo/TQwPum+Qh18om0aj0CMRzSabQmqou6uJHsRNTvfq/COdTqfX600mU3Cwu8+jdskIyuXyoKAgsVgcE/PwTo57N9olv2rNZlzZ8LRdC47jBoPB09Pz6ev0DmDxvOgJI/ihMRyHL1AoFJ9++uknn3zC5XKfvjkX5XoR3LVrF4fDeeGFFwja/zfffLN///6VK1dOnTqVoCY6e/DgQWNjY2hoqJ+fHwnNUZCLHQsaDIaGhgbi8tfY2Hjx4kW9Xr93716CmugiICAgJSUFw7B58+bpdDpyGqUUl4mgxWI5ePAgg8F49913iWtl3759NTU1AIC6urpjx44R11AXQqHwww8/JLNF6nCZCGZlZY0dO9a+sAFBGhoazp8/b/9aq9Xu2bOHuLYeFRcXN2fOHADAxo0byWwXOheIoEQiAQAUFRX5+PgQ2tChQ4dqa2s7vq2trT1y5AihLTo0atSoyZMnk98uLFSP4Nq1a9va2khoSCaTnT17tvMWrVb7888/k9B0F4MGDbJHv7i4mPzWyUfpCN6/fz8iImLw4MEktLV79257F9ixaiCGYfX19SQ0/Sj78QaTyVy0aBGUAshE0UEZg8FQVlYWFxfH5/NJblqhUOTk5BQUFJDcrkOlpaWxsbEWi8Xb2xt2LUShYi9oMBiys7NTUlLIzx8AwGq1xsfHk9+uQ6mpqTwer6amZsuWLbBrIQrlIqhWq2tqaoqKiphMJpQCzGazfVyGOlJTU7Va7a1bt2AXQghqRfDYsWMKhQJuJ6TX6wMDAyEW4NDSpUtDQ0NbW1tVKhXsWpyMQhGUyWRXr16Njo6GW4ZSqYTVAffMz8/Py8trxowZTU1NsGtxJqpEUCaTmUymjz76CHYhoLW1NTw8HHYVjtFotDNnzty6dYuaJ5FPhhIRXL9+PYZhkZGUWDpcIpFQfMbA888/j+P4t99+C7sQ54AfQaVSyeVyqTNtzmg0dswBoywajcbn8zsuJ7o0+OOCbW1tlBr0mjp16vfffx8SEgK7kN5VVVWFhYV5eHjALuSpwOwF9+/fv3HjRkrlr62tTSAQuET+AACxsbEMBuPll1+GXchTgRbBiooKPp+fm5sLqwCHiouLKXJI2kd0On3FihUUuZbzZKCtVR8fH0+dixAdLly4kJWVBbuKx5OUlBQWFga7iicHoRfUaDTz588nv92+UKlUo0ePhl3FY/Py8qqoqFiyZAnsQp4EhAiuXbt23bp15Lfbq4KCAi8vL6fcuES++Pj4FStWHDhwAHYhjw3+GTF15ObmLlq0aNiwYbALcS+k9oL37t376aefyGyx7yQSCYPB6Af5W79+Pcm3HDwlUiO4YcOGmTNnktli33333XezZs2CXYUTLF26lMFgwJps+wTQBzGwjxB9/PHHUKbpIyT1gjiOS6VSctp6Av/85z8JvTeUfDdu3Pj6669hV9EnJEVwx44dlD1ZO3z4sEgkIucOFdIMGTJEp9OdPn0adiG9I+mDeM2aNW+88QalrsXZWSyWCRMmuMQ/VX/l7seCb7755osvvpieng67EELU1taazebY2FjYhfSEjA/iS5culZaWktDQ49qxY0d0dHR/zZ99IcOFCxfq9XrYhfSEjAhu2rSJTqeT0NBjqaysvHr16ltvvQW7EGJt3bpVLBbDrqInhE9TwHE8LS0tOTmZ6IYeV05OzuXLl2FXQTiKfwq777Hg/PnzP/jgg4SEBNiFkGHbtm2RkZGUnQFE+AdxfX09lMWBerB+/fqcnBw3yR8AICUlZfv27bCr6BbhERSLxRcuXCC6lb7bvHkznU6fOHEi7ELIk5qa+uGHH1osFtiFOEZ4BEUi0bRp04hupY+OHj3a0NDw+uuvwy6EbOHh4YQuzfg03OhY8OrVq6dOnXr//fdhFwLBpUuXjh07tmbNGtiFOEB4LyiVSvPz84lupVe3b9/esGGDe+bPfjhYWVkJu4pu4ASrrq6eNWsW0a30rKqqavbs2XBrQLpD+PFBWFjYpEmTiG6lB1KpdMWKFQcPHoRYAxXYbDYMwzCMQo8Gsuvnx4KVlZXLly8/evQo7ELg++GHH6xWK9XumiXpJs78/Pz169ebTCa1Wh0QEEDaow0qKip2796N8mcXHh5+5coV2FU4QGAvOHr0aPuzXHAct/f/OI5nZ2evXbuWoBY7E4vFK1eupOwkRaQDgWfEzz33HI1Gs68bbt/i4eExYsQI4lrsUFZW9sMPP6D8dWY2mxsaGmBX4QCBEVy9enViYmLnXtbf3z8lJYW4Fu1KS0u/+OKLzz77jOiGXItSqfzTn/4EuwoHiB0X/PzzzzuWaMFxnMPhED1x4+LFi8eOHdu2bRuhrbgiNpttMFDxCfbERjAwMPDtt9+2rxiJYRjRXWBBQcGBAwdWrVpFaCsuytvb+/jx47CrcIDwqyMZGRkzZ87kcrk8Ho/QA8HDhw+fP3+emkuFUIRSScXH1/dpUMZituk1tiduY+7sV2vFD8RicXT4wPZWQuZrnD179s6v1dS8BkoRZrM5NzeXahPneh+UuVuivn1R1SI3efKeauZ9x7gMQUwmU0AoTybWRQ/iDRvvIwxx7YVHnWjFihWnT5/uGBSzHxHhOH7jxg3YpT3UUy9YcrJFITNnzgzi+1LxIQiPslnxtmZT/lb5uHmBwZFs2OVQwuuvv15eXm5/TkRHL0CpZTy7PRa8cqJF1WzJnBHoKvkDANDomG+Qx/QlEad3PWiqo+LZH/mio6OHDh3a+bMOwzBKraHoOIKtD0yKBuPIKQGk1+Mcz80NvnayFXYVVLFgwYLOz5MSiUR/+MMfoFb0PxxHUNFgxHHKTanoO74Ps75SZzI++SlUfxIbGzt8+HD71ziOZ2ZmUuoJZ44jqFFZ/cNc+1gqIpHb0miEXQVVvPTSSwEBAQCA0NDQnJwc2OX8D8cRNBttZoNrdyFqpQUAF+7InSsmJmbEiBE4jmdlZVGqC4S54j7SA5sNr6vQaVotWrXFYsb1WuvT7zMlZL5h8DNxvumndjnhKYpsTzrLk8YR0AU+zPB4ztPsCkWQWu6WqO9d10grdSEDBBYTTmfSaUwGwJwxKEFjD392stkGzDon7Kxdg1vNFqvFzGQaj34vi0jkDhjMi0t7kkeYowhSRfkVdeERhX84n8HlJ42n1mdlz3wifNsf6O5cNxTlKTOnC58Z/HhBRBGET6+x5m9pMltp0SNEDBbl1n/qFYZhgkAuAFyev+DamZa7VzWTFwXR6X09EIf/JE43V3dPu/3TWl6ob1CcvyvmrzOWJyM4MYDl471xpfhBfV8vDaAIwtRUbzh/sCVudISHp8tcguoVm8caOC4qf0uTWmnqy+tRBKGR3NGc3NkcluoaT/18XJHDRAc3yOW1vfeFKIJwaNosp3f12/zZRaaFHvy2wWLuZYAZRRCOE9ubIoeHwq6CcDEjQ/77Uy/DkCiCEFz7pdUKWAyma5989IUHl6XVYncuq3p4DYogBMX5yoBYH9hVkCQg2rcor6WHFzgzguV3y4zGp5oZcO78qbHZaXV1Nc4rinKun2oJTfSl4NouAIC/r52y/4iTb35leNCF4fyyS912hE6L4ImCvCVLFxoMlH6+ABXcvaphe7n2LKTH5cFjV1zTdPdTp0XwKfs/N6FuMRu0Nk++e93awhN6NtcbzN1M33TOBboTBXnrvvkMADB95jgAwHsrP/zdhKkAgJMn//vzri0ymVQo9Js8aUbOvFfsS3xYLJYtWzcWnDymUrVFREQtfPm1jPQxj+62uLhw0+ZvZTJpUFDItKm/nznjRadUC1H9PZ2PiEfQzquqr+f/skEmv8/n+cZGpU0c/7qA7wcAWPVp9qyp75XdPVd+r8iTzRs5bMbzYxfb32K1Wk+d+7H42mGTSR8TPdRsJupuB79Ifu1dXWyqg9/dOb3giOHpc2bPBwD849N1/1q3ecTwdABAQcGxf3z+4TPPxP911ZoxWeN/2vLvn/+zxf76L7/6ZM/eHVMmz/jg/z4JCgr569/evX37Zpd96nS61X9/j8VkLX9n1ahnRyuVzU4pFS5FoxnHCTkFrBRf/WH7m4EBUXOmfzB61LzqmpsbtywxmR5GavfBj0KCBryxaOOQlIknz/xQfq/Ivv3QsS9+Ofdj/IBRM6a8y2Ky9YZ2ImoDAFitWGuz44slzukFfXx8Q0JEAICEhCQvL2/7BPHNP32XnJy66v8+AQCMznyuvV29e8+2WTPnKhQPCk4eW/DS4oUvvwYAyBqdPX/BjK3bvv/6q42d99na1mI0GjMznxs/rv+sjq9VWRgenkTs+fB/vxqZNmPGlIePtB0QO+KLf714r6o4OXEMAGD4kGnZWQsBACFBA0quH7lfVZwYly6VVRRfO5Sd9crEcbkAgLTBk8USou7sZHowNN3cQk7UTBmptE6haH5xzksdW4YNezb/+BFpQ929e+UAgIyMsfbtGIYNSxv5y6mu61GHBIcOHDho588/stmeU6fMZLFYBJVKJr3G6uHj/OHAltbGpmaJoqW++NrhztvbVA+HhVmsh7mn0+leggCVuhkA8Gv5OQDA6FFzO16PYUQN0jE8aDo1uRHUaDUAAG9v344tfL4AAKBofqDVagAAPp1+JBB46XQ6rVbbeQ8Yhn225l+bf1y/8ft1+/bvfP+9v6ekDCGoWtIQtJhju0YJABg/dvGgxLGdt/P5fo++mEZj2GxWAEBbm5zN5nE5XoTU1AWO2br53Z2c+o77VQP8AwEAKlVbx49aW1vsQfTzCwAAqNW/DRS1tCgZDAab3XWogsfjvfXnv2zbeoDL5a366zv2BTNdGteLbjE6YRZ+F55sPgDAbDYG+Ed2/s+T3dOpD5frYzBozJY+zWd5Shajhe/juL9zWgQ92Z4AAIXi4UmDUOgXFBhcUlLU8YLz50+x2ezY2LiEhCQMw4qvFNq3m0ym4iuFAwcOotPpLCarczrtAz0hwaEzZ/xBo9XI5TJnVQsL34thMTk/gv5+4d5eQVdv5BlND8dlrVaLxWLu+V2i0HgAwM3bBU6v51EWk5Xv7TiC9NWrVz+6tUGst1pAUORjHDizPTlHju6rqa3GAFZ+99e4uEQ+T7Bn387m5iaz2Xzw0O5Tp4/nzHt1WNpIAV8glzceOrwHAEyhaP73v/8pqRGvePdvwcGhDCbz0OE9FffuhIdH+gn9FyycqVA0K5WKQ4f3mIzGRa++0fdHCFXeVEcmcHjd/NqwaFRmpdzi6e3kMxIMw3y8g0uuHy2vuIgDvLb+10PHvrJaTRFhyQCAMxe3i0Li42IfLmtWfPUwm80dPOj5AL+o23dOX7+ZrzdoNNrWy1cPiSXXRCEJifEZzi0PAGBQaaMS2b6BDg7onRZBAV/g7x947twvly9fbG9XT5gwJTZ2gI+P75mzJ4+fONrW2jJv3ivzc161X5galvasVqs5fuLImTMFXA733eWrhg17FgDA5/GDg0Ju3LxKw2gJiclSaV1h0dmLhWeEQv+/rFwdGirqez3UjCBHwCj5r0IY4fzDr0D/SFFoYnVN6fXS/DrpneDg2KGpE+3jgt1FkEajJQzIaFbU3r5zurqmNCgguqVVFugfRUQEJdebxuUE0mgOLks6XlmrpKDFZAApY3wf/ZGryP9RmjXTL4h6ixv9Z229d7iQ4+VGF0jaFTqLun3GEseTI6nVSbiDxJG8qjv6HiJ4v6pk+x4HDyrzZPO7GzqeMmHZyLTpzqrw7r2in/f/7dHtOI4DgDscuMl95TtRSHx3OzRqjAOHc7v7KYog2VJH+1w+JvYRCegMx+eCkeGD3nljx6PbcRx0N72G4+nMT/aYqKEOC7DZbDiO0+kOxjUFfP/u9mbSm9VyTcKwbpeTQxGEIH2qsPx6S1Ccg0E7AACLxfZlwZzQ79wCFNWtmdOFPbwATVmFYFCmtyfbatT3MmjSDxjajd5CrOeb21EE4Zj4SlB1MRUfRONENhteXSKb9EpQzy9DEYSD5UGb/nqIpKQ/p7C6WDp3ZXivL0MRhCY4ynPm0iBJiRR2Ic5ntdgqi+rmvSfyCeh9cgmKIExeQtbUxUFlJyV6df9ZGVvbaqgsrHvxHRGH16eTXRRByPxCPZZ8HWPTqBvKmoxaMmYMEEevNtbfamTaNLmfxwj6vEo+GpSBD8OwyYuCJWXaC4cecLzZDI6HwJ9Dd527jC1Gq7pZazWazFrjmJl+YQMeb8VLFEGqiEriRiVxxb9qKm9qq4pafEUcs9FGZzEYHgwKrliM47jVaLGaLUwWrVWuj0riPpPOi0x8kmURUQSpJSaZF5PMAwA0SvRalVWrspiMNoMzFvp1Lg8Ojc1hcQQcvg89MLyXYZeeoQhSVHAUIbeYUJDjCLLYmI16nf9j8fJnEnYjBOJMjv+V+D7M5lrXXhdBclsjDO4Pdzz1e44jGBDmQck1T/qqrdkUOZDDYKJu0AV02wuGxrJWfsvyAAAAkklEQVQvHJCTXo9znP5ZNnJST7MzEOro6XnEdy6rKks1KVlCn0BWd5PbKEWvsagU5gv75bOWhXr34dIQQgW9PBJbckdber5NLjHQGVT/YPYN9lA1m6KTOMMnCrkCdKbvMnqJYAejnuqPpMNxwOa4QFeNdNHXCCIIQVC3gUCGIohAhiKIQIYiiECGIohAhiKIQPb/UNqA9V2eTi4AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggered Tool or Agent: get_news\n",
      "Here are the latest updates on the Delhi elections:\n",
      "\n",
      "1. **Economic Times**: Delhi Police have registered a case against the son of Ramesh Bidhuri over a violation of the Model Code of Conduct (MCC), following allegations from Atishi regarding interference during the silence period. AAP members have clashed with police, and the party has developed a website to monitor votes and tally polling data. Arvind Kejriwal has raised concerns about the BJP potentially causing discrepancies in 10% of the votes through machines.\n",
      "   [Read more](https://economictimes.indiatimes.com/news/newsblogs/delhi-elections-2025-live-updates-date-time-constituencies-parties-manifesto-arvind-kejriwal-atishi-paresh-verma-sandeep-dikshit-aap-bjp-congress-delhi-assembly-elections-eci-latest-news-4-february/liveblog/117899103.cms)\n",
      "\n",
      "2. **Times of India**: Delhi Chief Minister Atishi accused the BJP of engaging in disruptive activities during the election period.\n",
      "   [Read more](https://timesofindia.indiatimes.com/india/delhi-elections-2025-live-updates-arvind-kejriwal-aap-bjp-congress-manifesto-candidates-list-india-election-commission-assembly-poll/liveblog/117900187.cms)\n",
      "\n",
      "3. **Hindustan Times**: Approximately 1.56 crore voters are eligible to cast their votes across 13,766 polling stations on February 5, in the upcoming elections featuring AAP, Congress, and BJP.\n",
      "   [Read more](https://www.hindustantimes.com/india-news/delhi-election-2025-live-updates-assembly-aap-bjp-arvind-kejriwal-poll-results-eci-amit-shah-rahul-gandhi-101738645285001.html)\n",
      "\n",
      "4. **Business Standard**: Major news developments related to the Delhi elections are being covered live, including public meetings and speeches from key candidates.\n",
      "   [Read more](https://www.business-standard.com/elections/assembly-election/delhi-assembly-elections-live-updates-aap-arvind-kejriwal-bjp-congress-delhi-polls-time-date-when-125020400211_1.html)\n",
      "\n",
      "Stay tuned for more updates as the elections approach!\n"
     ]
    }
   ],
   "source": [
    "# Import the keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "import requests\n",
    "from typing import List, Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WEATHER_API_KEY = os.environ['WEATHER_API_KEY']\n",
    "TAVILY_API_KEY = os.environ['TAVILY_API_KEY']\n",
    "\n",
    "# Retrieve credentials from environment variables\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "azure_openai_endpoint = os.getenv(\"ENDPOINT_URL_MINI\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_MINI\")\n",
    "deployment_name = os.getenv('DEPLOYMENT_NAME_MINI')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_api_key\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# Define the tools\n",
    "@tool\n",
    "def get_weather(query: str) -> list:\n",
    "    \"\"\"Search weatherapi to get the current weather\"\"\"\n",
    "    endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={query}\"\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "    if data.get(\"location\"):\n",
    "        return data\n",
    "    else:\n",
    "        return \"Weather Data Not Found\"\n",
    "\n",
    "# @tool\n",
    "# def get_news(query: str) -> list:\n",
    "#     \"\"\"Search the news from web\"\"\"\n",
    "#     tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4, search_depth='advanced', max_tokens=1000)\n",
    "#     results = tavily_search.invoke(query)\n",
    "#     return results\n",
    "\n",
    "@tool\n",
    "def get_news(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for news articles related to the query and return the top results with citation links.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query for news articles.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string containing the top news results with titles, snippets, and citation links.\n",
    "    \"\"\"\n",
    "    # Initialize TavilySearchResults\n",
    "    tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4, search_depth='advanced', max_tokens=1000)\n",
    "    \n",
    "    # Fetch search results\n",
    "    results = tavily_search.invoke(query)\n",
    "    \n",
    "    # Format the results with titles, snippets, and URLs\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        title = result.get(\"title\", \"No Title\")\n",
    "        snippet = result.get(\"content\", \"No Snippet Available\")\n",
    "        url = result.get(\"url\", \"No URL Available\")\n",
    "        \n",
    "        # Format each result\n",
    "        formatted_result = f\"**Title:** {title}\\n**Snippet:** {snippet}\\n**Link:** {url}\\n\"\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    # Combine all results into a single string\n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "@tool\n",
    "def translate_text(query: str) -> str:\n",
    "    \"\"\"Translates the given query text into the target language.\"\"\"\n",
    "    prompt = f'''You are an efficient translator. First, analyze the query and its language, then translate it to the target language.\n",
    "    ## Given Query: {query}\n",
    "    Note: In the output, only provide the translation of the query, nothing else.\n",
    "    ## Desired Output:\n",
    "    translated text: \"your translated text will be here\"\n",
    "    Ensure you follow the output format.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates the given sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    stream = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return stream.choices[0].message.content\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(query: str) -> str:\n",
    "    \"\"\"Performs sentiment analysis on the provided query.\"\"\"\n",
    "    prompt = f'''\n",
    "    You are a sentiment analyzer. First, analyze the sentiment of the given query and detect its language. Then, provide the sentiment analysis result.\n",
    "    ## Given Query: {query}\n",
    "    Note: In the output, only provide the sentiment result, nothing else.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs sentiment analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    stream = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return stream.choices[0].message.content\n",
    "\n",
    "@tool\n",
    "def get_financial_data(query: str) -> str:\n",
    "    \"\"\"Simulates fetching financial data for a given query.\"\"\"\n",
    "    # Simulate financial data retrieval\n",
    "    return f\"Financial data for {query} is not implemented yet.\"\n",
    "\n",
    "# List of tools\n",
    "tools = [get_weather, get_news, translate_text, analyze_sentiment, get_financial_data]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Intent detection and entity extraction\n",
    "def detect_intent_and_entities(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Detects the intent and extracts entities from the user query.\n",
    "    \"\"\"\n",
    "    prompt = f'''\n",
    "    Analyze the following query and determine the intent and extract relevant entities.\n",
    "    Query: {query}\n",
    "    Possible Intents: finance, weather, news, sentiment_analysis, translation\n",
    "    Entities to Extract: companies, locations, keywords\n",
    "    Output Format:\n",
    "    {{\n",
    "        \"intent\": \"detected_intent\",\n",
    "        \"entities\": {{\n",
    "            \"companies\": [\"list\", \"of\", \"companies\"],\n",
    "            \"locations\": [\"list\", \"of\", \"locations\"],\n",
    "            \"keywords\": [\"list\", \"of\", \"keywords\"]\n",
    "        }}\n",
    "    }}\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that detects intent and extracts entities.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return eval(response.choices[0].message.content)\n",
    "\n",
    "# Tool routing logic\n",
    "def route_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Routes the query to the appropriate tool based on the detected intent.\n",
    "    If no intent or entities are relevant, the LLM handles the query directly.\n",
    "    \"\"\"\n",
    "    intent_and_entities = detect_intent_and_entities(query)\n",
    "    intent = intent_and_entities[\"intent\"]\n",
    "    entities = intent_and_entities[\"entities\"]\n",
    "\n",
    "    if intent == \"finance\":\n",
    "        return get_financial_data(query)\n",
    "    elif intent == \"weather\":\n",
    "        return get_weather(entities[\"locations\"][0])\n",
    "    elif intent == \"news\":\n",
    "        return get_news(entities[\"keywords\"][0])\n",
    "    elif intent == \"sentiment_analysis\":\n",
    "        return analyze_sentiment(query)\n",
    "    elif intent == \"translation\":\n",
    "        return translate_text(query)\n",
    "    else:\n",
    "        # If no intent is detected, let the LLM handle the query directly\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# StateGraph setup\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    check_tool = last_message.tool_calls\n",
    "    if check_tool:\n",
    "        print(\"Triggered Tool or Agent:\", check_tool[0]['name'])\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# Initialize the workflow\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"LLM\", call_model)\n",
    "workflow.add_edge(START, \"LLM\")\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_conditional_edges(\"LLM\", call_tools)\n",
    "workflow.add_edge(\"tools\", \"LLM\")\n",
    "agent = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the agent\n",
    "query = input(\"Enter your Query: \")\n",
    "result = agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "result = result[\"messages\"][-1].content\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "query = input(\"Enter your Query: \")\n",
    "result = agent.invoke({\"messages\": [(\"user\", query)]})\n",
    "result = result[\"messages\"][-1].content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Challenge (Optional) - cached memory\n",
    "For advanced candidates, allow memory to persist across multiple interactions, so a user\n",
    "does not lose context even if they exit and return later.\n",
    "\n",
    "Exact Match Checking: The check_history() function looks for identical previous queries\n",
    "\n",
    "Cached Responses: Immediately returns previous responses for identical queries\n",
    "\n",
    "Bypass Processing: Skips LLM/tool invocation for repeated queries\n",
    "\n",
    "How this works:\n",
    "\n",
    "Checks all previous human messages for exact matches\n",
    "\n",
    "Returns the immediately following AI response if found\n",
    "\n",
    "Maintains original behavior for new queries\n",
    "\n",
    "Works across sessions due to persistent memory\n",
    "\n",
    "Note: This implements exact string matching. For more sophisticated matching (semantic similarity), you'd need to add vector search capabilities, but this version handles literal repeats efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded previous conversation history.\n",
      "\n",
      "\n",
      "Triggered Tool or Agent: translate_text\n",
      "\n",
      "Assistant: The phrase \"I am happy\" translates to French as **\"Je suis heureux\"** (for a male speaker) or **\"Je suis heureuse\"** (for a female speaker).\n",
      "\n",
      "Regarding sentiment, the analysis indicates that the sentiment around the phrase \"I am happy\" is **positive**.\n",
      "\n",
      "Assistant (cached): It seems that your message was empty. How can I assist you further?\n",
      "\n",
      "Conversation history saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "import requests\n",
    "from typing import List, Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve credentials\n",
    "WEATHER_API_KEY = os.environ['WEATHER_API_KEY']\n",
    "TAVILY_API_KEY = os.environ['TAVILY_API_KEY']\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "azure_openai_endpoint = os.getenv(\"ENDPOINT_URL_MINI\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_MINI\")\n",
    "deployment_name = os.getenv('DEPLOYMENT_NAME_MINI')\n",
    "\n",
    "# Configure Azure OpenAI clients\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_api_key\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# Initialize persistent memory\n",
    "memory_file = \"conversation_memory.json\"\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Load previous conversation history\n",
    "if os.path.exists(memory_file):\n",
    "    try:\n",
    "        with open(memory_file, \"r\") as f:\n",
    "            saved_messages = json.load(f)\n",
    "            for msg_data in saved_messages:\n",
    "                if msg_data[\"type\"] == \"human\":\n",
    "                    memory.chat_memory.add_user_message(msg_data[\"content\"])\n",
    "                elif msg_data[\"type\"] == \"ai\":\n",
    "                    memory.chat_memory.add_ai_message(msg_data[\"content\"])\n",
    "        print(\"\\nLoaded previous conversation history.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading memory: {e}\")\n",
    "\n",
    "# Define the tools\n",
    "@tool\n",
    "def get_weather(query: str) -> list:\n",
    "    \"\"\"Search weatherapi to get the current weather\"\"\"\n",
    "    endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={query}\"\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "    if data.get(\"location\"):\n",
    "        return data\n",
    "    else:\n",
    "        return \"Weather Data Not Found\"\n",
    "\n",
    "# @tool\n",
    "# def get_news(query: str) -> list:\n",
    "#     \"\"\"Search the news from web\"\"\"\n",
    "#     tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4, search_depth='advanced', max_tokens=1000)\n",
    "#     results = tavily_search.invoke(query)\n",
    "#     return results\n",
    "\n",
    "@tool\n",
    "def get_news(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the web for news articles related to the query and return the top results with citation links.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query for news articles.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string containing the top news results with titles, snippets, and citation links.\n",
    "    \"\"\"\n",
    "    # Initialize TavilySearchResults\n",
    "    tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4, search_depth='advanced', max_tokens=1000)\n",
    "    \n",
    "    # Fetch search results\n",
    "    results = tavily_search.invoke(query)\n",
    "    \n",
    "    # Format the results with titles, snippets, and URLs\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        title = result.get(\"title\", \"No Title\")\n",
    "        snippet = result.get(\"content\", \"No Snippet Available\")\n",
    "        url = result.get(\"url\", \"No URL Available\")\n",
    "        \n",
    "        # Format each result\n",
    "        formatted_result = f\"**Title:** {title}\\n**Snippet:** {snippet}\\n**Link:** {url}\\n\"\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    # Combine all results into a single string\n",
    "    return \"\\n\\n\".join(formatted_results)\n",
    "\n",
    "@tool\n",
    "def translate_text(query: str) -> str:\n",
    "    \"\"\"Translates the given query text into the target language.\"\"\"\n",
    "    prompt = f'''You are an efficient translator. First, analyze the query and its language, then translate it to the target language.\n",
    "    ## Given Query: {query}\n",
    "    Note: In the output, only provide the translation of the query, nothing else.\n",
    "    ## Desired Output:\n",
    "    translated text: \"your translated text will be here\"\n",
    "    Ensure you follow the output format.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates the given sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    stream = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return stream.choices[0].message.content\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(query: str) -> str:\n",
    "    \"\"\"Performs sentiment analysis on the provided query.\"\"\"\n",
    "    prompt = f'''\n",
    "    You are a sentiment analyzer. First, analyze the sentiment of the given query and detect its language. Then, provide the sentiment analysis result.\n",
    "    ## Given Query: {query}\n",
    "    Note: In the output, only provide the sentiment result, nothing else.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs sentiment analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    stream = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return stream.choices[0].message.content\n",
    "\n",
    "@tool\n",
    "def get_financial_data(query: str) -> str:\n",
    "    \"\"\"Simulates fetching financial data for a given query.\"\"\"\n",
    "    # Simulate financial data retrieval\n",
    "    return f\"Financial data for {query} is not implemented yet.\"\n",
    "\n",
    "\n",
    "\n",
    "# Configure LangGraph workflow\n",
    "tools = [get_weather, get_news, translate_text, analyze_sentiment, get_financial_data]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def detect_intent(query: str) -> dict:\n",
    "    \"\"\"Detect intent using Azure OpenAI\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Analyze this query and output JSON:\n",
    "            Query: {query}\n",
    "            Output format: {{\n",
    "                \"intent\": \"weather/news/finance/sentiment/translation/general\",\n",
    "                \"entities\": {{\n",
    "                    \"locations\": [],\n",
    "                    \"keywords\": [],\n",
    "                    \"companies\": []\n",
    "                }}\n",
    "            }}\"\"\"\n",
    "        }],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return eval(response.choices[0].message.content)\n",
    "\n",
    "def route_query(query: str) -> str:\n",
    "    \"\"\"Enhanced routing with memory context\"\"\"\n",
    "    intent_data = detect_intent(query)\n",
    "    \n",
    "    if intent_data[\"intent\"] == \"weather\":\n",
    "        return get_weather(intent_data[\"entities\"][\"locations\"][0])\n",
    "    elif intent_data[\"intent\"] == \"news\":\n",
    "        return get_news(\" \".join(intent_data[\"entities\"][\"keywords\"]))\n",
    "    elif intent_data[\"intent\"] == \"finance\":\n",
    "        return get_financial_data(intent_data[\"entities\"][\"companies\"][0])\n",
    "    elif intent_data[\"intent\"] == \"translation\":\n",
    "        return translate_text(query)\n",
    "    elif intent_data[\"intent\"] == \"sentiment\":\n",
    "        return analyze_sentiment(query)\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Configure StateGraph\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    check_tool = last_message.tool_calls\n",
    "    if check_tool:\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"Triggered Tool or Agent:\", check_tool[0]['name'])\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"LLM\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"LLM\")\n",
    "workflow.add_conditional_edges(\"LLM\", call_tools)\n",
    "workflow.add_edge(\"tools\", \"LLM\")\n",
    "agent = workflow.compile()\n",
    "\n",
    "\n",
    "def check_history(query: str) -> str | None:\n",
    "    \"\"\"Check for exact query matches in history\"\"\"\n",
    "    history = memory.load_memory_variables({})['history']\n",
    "    messages = memory.chat_memory.messages\n",
    "    \n",
    "    for i in range(0, len(messages), 2):\n",
    "        if i+1 >= len(messages):\n",
    "            break\n",
    "        if isinstance(messages[i], HumanMessage) and messages[i].content == query:\n",
    "            return messages[i+1].content  # Return next message (AI response)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Main interaction loop with persistent memory\n",
    "try:\n",
    "    while True:\n",
    "        query = input(\"\\nYou: \")\n",
    "        if query.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        \n",
    "        # Check for existing response in history\n",
    "        cached_response = check_history(query)\n",
    "        if cached_response:\n",
    "            print(f\"\\nAssistant (cached): {cached_response}\")\n",
    "            continue\n",
    "            \n",
    "        # Rest of original processing flow\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        messages = []\n",
    "        for msg in memory.chat_memory.messages:\n",
    "            role = \"user\" if isinstance(msg, HumanMessage) else \"assistant\"\n",
    "            messages.append((role, msg.content))\n",
    "        \n",
    "        response = agent.invoke({\"messages\": messages})\n",
    "        ai_response = response[\"messages\"][-1].content\n",
    "        \n",
    "        memory.chat_memory.add_ai_message(ai_response)\n",
    "        print(f\"\\nAssistant: {ai_response}\")\n",
    "\n",
    "finally:\n",
    "    # Save conversation history\n",
    "    try:\n",
    "        with open(memory_file, \"w\") as f:\n",
    "            history = [{\n",
    "                \"type\": \"human\" if isinstance(msg, HumanMessage) else \"ai\",\n",
    "                \"content\": msg.content\n",
    "            } for msg in memory.chat_memory.messages]\n",
    "            json.dump(history, f, indent=2)\n",
    "        print(\"\\nConversation history saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving memory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek = evaluation answers\n",
    "\n",
    "1. **Routing Logic & Intent Detection**:\n",
    "\n",
    "- The code uses `detect_intent` which sends a prompt to Azure OpenAI to classify the query into an intent and extract entities. But how accurate is this? Maybe there's no validation if entities exist before using them. For example, if \"locations\" is empty but the intent is \"weather\", it would crash when accessing `intent_data[\"entities\"][\"locations\"][0]`.\n",
    "\n",
    "2. **Agent Framework Implementation**:\n",
    "\n",
    "- LangGraph's StateGraph is set up with nodes and edges. The agent uses a workflow with LLM and tools. But does it handle tool execution properly? The `call_tools` function checks for tool calls and routes accordingly. But maybe there's no error handling if a tool fails.\n",
    "\n",
    "3. **Memory Usage**:\n",
    "\n",
    "- ConversationBufferMemory is used with persistent storage in a JSON file. It loads previous conversations and saves on exit. The `check_history` function looks for exact matches. However, this might not handle similar but not identical queries. Also, does the memory integrate with the agent's state correctly?\n",
    "\n",
    "4. **Code Quality**:\n",
    "\n",
    "- The code is structured with functions and tools, but some parts could be more modular. For example, the `route_query` function directly calls tools without error handling. Also, the `detect_intent` uses `eval` on the response, which is a security risk if the model's output isn't controlled.\n",
    "\n",
    "5. **Handling Edge Cases**:\n",
    "\n",
    "- Tools like `get_weather` check if the data has a \"location\" key, but others like `get_news` don't handle empty results. If Tavily returns no results, it might still try to format them, causing errors. Also, intent detection might misclassify, leading to incorrect tool calls without fallbacks.\n",
    "\n",
    "6. **Bonus (Multi-step Queries)**:\n",
    "\n",
    "- The current code routes to a single tool per query. For a query like \"Stock price of Apple and sentiment around it\", the agent would need to call `get_financial_data` and `analyze_sentiment` sequentially. The existing setup doesn't support chaining tools or handling multi-intent queries.\n",
    "\n",
    "**Improvement Steps**:\n",
    "\n",
    "- **Routing & Intent**: Add validation for entities, handle empty cases, and improve the intent detection prompt for accuracy. Maybe use a schema with Pydantic for safer parsing.\n",
    "\n",
    "- **Agent Framework**: Implement error handling for tool executions, add retries or fallback responses.\n",
    "\n",
    "- **Memory**: Enhance `check_history` to consider semantic similarity, not just exact matches. Use memory within the agent's state to maintain context for multi-step queries.\n",
    "\n",
    "- **Code Quality**: Refactor into classes or separate modules for better organization. Replace `eval` with JSON parsing and validation.\n",
    "\n",
    "- **Edge Cases**: Add try/except blocks around tool calls, provide user-friendly error messages, handle empty results from APIs.\n",
    "\n",
    "- **Multi-step Queries**: Modify the agent workflow to allow chaining tools. For example, after getting financial data, automatically analyze sentiment if needed. Use the LLM to decide if multiple tools are required and orchestrate their execution.\n",
    "\n",
    "**Testing Plan**:\n",
    "\n",
    "- Unit tests for each tool, checking responses and error cases.\n",
    "\n",
    "- Integration tests for the agent workflow with different query types.\n",
    "\n",
    "- Test multi-step queries to ensure tools are called in sequence.\n",
    "\n",
    "- Validate memory persistence across sessions and correct retrieval of cached responses.\n",
    "\n",
    "**Potential Risks**:\n",
    "\n",
    "- Security issues with `eval` in `detect_intent`.\n",
    "\n",
    "- Tools failing and crashing the agent without feedback.\n",
    "\n",
    "- Memory growing too large without pruning, leading to performance issues.\n",
    "\n",
    "- Incorrect routing leading to irrelevant tool calls or responses.\n",
    "\n",
    "**Final Steps**:\n",
    "\n",
    "Implement the identified improvements step by step, test each part, and ensure the evaluation criteria are met. Focus on robustness, accuracy, and user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT - evaluation answer\n",
    "\n",
    "Evaluating your AI agent involves assessing various aspects to ensure it functions effectively and efficiently. Here's a structured evaluation based on the provided criteria:\n",
    "\n",
    "1. Routing Logic & Intent Detection\n",
    "\n",
    "Assessment: The detect_intent function utilizes Azure OpenAI to classify user queries into predefined intents such as \"weather,\" \"news,\" \"finance,\" \"sentiment,\" \"translation,\" or \"general.\" This classification directs the query to the appropriate function for processing.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Accuracy: Ensure that the intent detection accurately classifies a diverse set of user inputs. Consider testing with various phrasings and edge cases to validate robustness.\n",
    "\n",
    "Scalability: If new intents are added in the future, assess how easily the system can accommodate them. The current structure should allow for straightforward expansion.\n",
    "\n",
    "2. Agent Framework Implementation\n",
    "\n",
    "Assessment: The code employs the LangGraph framework to manage the workflow between the language model and the tools. The StateGraph is configured to handle the flow of messages and tool invocations.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Modularity: The separation of concerns is well-maintained, with distinct functions for each tool and clear workflow management.\n",
    "\n",
    "Integration: The binding of tools to the language model (llm_with_tools) is effectively implemented, allowing seamless tool invocation during interactions.\n",
    "\n",
    "3. Memory Usage\n",
    "\n",
    "Assessment: The ConversationBufferMemory is used to maintain a history of interactions, which is loaded from and saved to a JSON file (conversation_memory.json). This enables context retention across sessions.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Persistence: The mechanism for loading and saving conversation history ensures that the agent can recall previous interactions, enhancing user experience.\n",
    "\n",
    "Efficiency: Consider the potential growth of the memory file over time. Implementing strategies for memory management, such as summarizing older interactions, could be beneficial.\n",
    "\n",
    "4. Code Quality\n",
    "\n",
    "Assessment: The code is structured with clear function definitions, appropriate use of comments, and logical flow.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Readability: The use of descriptive function names and comments aids in understanding the code's purpose and functionality.\n",
    "\n",
    "Maintainability: The modular design allows for easy updates and debugging. However, ensure that exception handling is consistently implemented across all functions to enhance robustness.\n",
    "\n",
    "5. Handling Edge Cases\n",
    "\n",
    "Assessment: The code includes basic error handling, such as checking for the existence of location data in the get_weather function.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Graceful Degradation: In scenarios where data is unavailable or an error occurs, the system should provide informative messages to the user. Expanding error handling to cover all functions will improve resilience.\n",
    "\n",
    "Unexpected Inputs: Test the system with a variety of unexpected inputs to ensure it responds appropriately without crashing or producing erroneous outputs.\n",
    "\n",
    "6. Bonus: Handling Multi-Step Queries\n",
    "\n",
    "Assessment: The current implementation routes queries to specific functions based on detected intent but does not inherently support multi-step query handling.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Sequential Tool Invocation: To handle queries like \"Stock price of Apple and sentiment around it,\" the system would need to invoke multiple tools in sequence. Implementing a mechanism to parse such queries and manage sequential tool calls would enhance functionality.\n",
    "Recommendations:\n",
    "\n",
    "Enhance Intent Detection: Consider incorporating more advanced natural language understanding techniques or training a custom model to improve intent detection accuracy.\n",
    "\n",
    "Expand Error Handling: Implement comprehensive exception handling across all functions to manage potential failures gracefully.\n",
    "\n",
    "Implement Multi-Step Query Handling: Develop a strategy to parse and execute multi-step queries, possibly by decomposing them into individual intents and processing them sequentially.\n",
    "\n",
    "Optimize Memory Management: As the conversation history grows, consider methods to manage memory usage, such as summarizing older interactions or setting retention limits.\n",
    "\n",
    "By addressing these areas, the AI agent will become more robust, user-friendly, and capable of handling a wider range of user interactions effectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded previous conversation history.\n",
      "\n",
      "Assistant: 2 + 2 equals 4.\n",
      "\n",
      "Assistant: ⚠️ System error: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_NezKxHXNM1tYm8pLG9kzpgN9\", 'type': 'invalid_request_error', 'param': 'messages.[22].role', 'code': None}}\n",
      "\n",
      "=== Evaluation Report ===\n",
      "Success Rate: 50.0%\n",
      "Avg Response Time: 16.14s\n",
      "Tool Usage:\n",
      "  - get_weather: 0\n",
      "  - get_news: 0\n",
      "  - translate_text: 0\n",
      "  - analyze_sentiment: 0\n",
      "  - get_financial_data: 0\n",
      "Error Rate: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "import requests\n",
    "from typing import List, Literal, Tuple, Dict, Any\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve credentials\n",
    "WEATHER_API_KEY = os.environ['WEATHER_API_KEY']\n",
    "TAVILY_API_KEY = os.environ['TAVILY_API_KEY']\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "azure_openai_endpoint = os.getenv(\"ENDPOINT_URL_MINI\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_MINI\")\n",
    "deployment_name = os.getenv('DEPLOYMENT_NAME_MINI')\n",
    "\n",
    "# Configure Azure OpenAI clients\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_api_key\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "# Initialize persistent memory and encoder for semantic caching\n",
    "memory_file = \"conversation_memory.json\"\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "memory = ConversationBufferWindowMemory(k=6, return_messages=True)\n",
    "\n",
    "# Load previous conversation history\n",
    "if os.path.exists(memory_file):\n",
    "    try:\n",
    "        with open(memory_file, \"r\") as f:\n",
    "            saved_messages = json.load(f)\n",
    "            for msg_data in saved_messages:\n",
    "                if msg_data[\"type\"] == \"human\":\n",
    "                    memory.chat_memory.add_user_message(msg_data[\"content\"])\n",
    "                elif msg_data[\"type\"] == \"ai\":\n",
    "                    memory.chat_memory.add_ai_message(msg_data[\"content\"])\n",
    "        print(\"\\nLoaded previous conversation history.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading memory: {e}\")\n",
    "\n",
    "# Define tools with error handling\n",
    "@tool\n",
    "def get_weather(query: str) -> str:\n",
    "    \"\"\"Get current weather using weatherapi\"\"\"\n",
    "    try:\n",
    "        endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={query}\"\n",
    "        response = requests.get(endpoint, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return f\"{data['location']['name']}: {data['current']['temp_c']}°C\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Weather service error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_news(query: str) -> str:\n",
    "    \"\"\"Get news articles with Tavily\"\"\"\n",
    "    try:\n",
    "        tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4)\n",
    "        results = tavily_search.invoke(query)\n",
    "        return \"\\n\\n\".join(\n",
    "            f\"**{res['title']}**\\n{res['content']}\\nLink: {res['url']}\"\n",
    "            for res in results\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ News service error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def translate_text(query: str) -> str:\n",
    "    \"\"\"Translate text using Azure OpenAI\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate this to English: {query}. Respond only with the translation.\"\n",
    "            }],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Translation error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(query: str) -> str:\n",
    "    \"\"\"Analyze sentiment using Azure OpenAI\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Analyze sentiment: {query}. Respond with ONLY the sentiment label (positive/negative/neutral).\"\n",
    "            }],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Sentiment analysis error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def get_financial_data(query: str) -> str:\n",
    "    \"\"\"Fetch financial data\"\"\"\n",
    "    try:\n",
    "        # Simulated financial data\n",
    "        return f\"📈 {query} (Simulated)\\nOpen: $150\\nHigh: $155\\nLow: $148\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Financial service error: {str(e)}\"\n",
    "\n",
    "# Intent detection model\n",
    "class IntentData(BaseModel):\n",
    "    intents: List[str]\n",
    "    entities: Dict[str, List[str]]\n",
    "\n",
    "def detect_intent(query: str) -> IntentData:\n",
    "    \"\"\"Detect multiple intents from query\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Analyze query and output JSON with:\n",
    "                - intents: list of detected intents (weather/news/finance/sentiment/translation)\n",
    "                - entities: {locations, keywords, companies}\"\"\"\n",
    "            }, {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }]\n",
    "        )\n",
    "        return IntentData.parse_raw(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Intent detection error: {e}\")\n",
    "        return IntentData(intents=[\"general\"], entities={})\n",
    "\n",
    "# Evaluation metrics tracker\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self):\n",
    "        self.total_queries = 0\n",
    "        self.successful_responses = 0\n",
    "        self.response_times = []\n",
    "        self.tool_usage = {tool.name: 0 for tool in tools}\n",
    "        self.error_count = 0\n",
    "\n",
    "    def track_query(self, success: bool, response_time: float):\n",
    "        self.total_queries += 1\n",
    "        if success:\n",
    "            self.successful_responses += 1\n",
    "        self.response_times.append(response_time)\n",
    "\n",
    "    def track_tool_usage(self, tool_name: str):\n",
    "        if tool_name in self.tool_usage:\n",
    "            self.tool_usage[tool_name] += 1\n",
    "\n",
    "    def track_error(self):\n",
    "        self.error_count += 1\n",
    "\n",
    "    def generate_report(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"success_rate\": self.successful_responses / self.total_queries if self.total_queries else 0,\n",
    "            \"avg_response_time\": np.mean(self.response_times) if self.response_times else 0,\n",
    "            \"tool_usage\": self.tool_usage,\n",
    "            \"error_rate\": self.error_count / self.total_queries if self.total_queries else 0\n",
    "        }\n",
    "\n",
    "# Configure LangGraph workflow\n",
    "tools = [get_weather, get_news, translate_text, analyze_sentiment, get_financial_data]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "metrics = EvaluationMetrics()\n",
    "\n",
    "class FinancialAgent:\n",
    "    def __init__(self):\n",
    "        self.workflow = self._build_workflow()\n",
    "        self.metrics = EvaluationMetrics()\n",
    "\n",
    "    def _build_workflow(self):\n",
    "        workflow = StateGraph(MessagesState)\n",
    "        \n",
    "        workflow.add_node(\"LLM\", self.call_model)\n",
    "        workflow.add_node(\"tools\", self.handle_tools)\n",
    "        workflow.add_node(\"multi_step\", self.handle_multi_step)\n",
    "        \n",
    "        workflow.add_edge(START, \"LLM\")\n",
    "        workflow.add_conditional_edges(\n",
    "            \"LLM\",\n",
    "            self.route_processing\n",
    "        )\n",
    "        workflow.add_conditional_edges(\n",
    "            \"tools\",\n",
    "            lambda state: END if \"error\" in state else \"LLM\"\n",
    "        )\n",
    "        workflow.add_edge(\"multi_step\", \"LLM\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "\n",
    "    def call_model(self, state: MessagesState):\n",
    "        messages = state[\"messages\"]\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    def handle_tools(self, state: MessagesState):\n",
    "        try:\n",
    "            return ToolNode(tools)(state)\n",
    "        except Exception as e:\n",
    "            return {\"messages\": [AIMessage(content=f\"⚠️ Tool error: {str(e)}\")]}\n",
    "\n",
    "    def handle_multi_step(self, state: MessagesState):\n",
    "        messages = state[\"messages\"]\n",
    "        query = messages[-1].content\n",
    "        intents = detect_intent(query)\n",
    "        \n",
    "        results = []\n",
    "        for intent in intents.intents:\n",
    "            result = self.route_single_intent(intent, intents.entities)\n",
    "            results.append(result)\n",
    "            self.metrics.track_tool_usage(intent)\n",
    "        \n",
    "        return {\"messages\": [AIMessage(content=\"\\n\\n\".join(results))]}\n",
    "\n",
    "    def route_processing(self, state: MessagesState):\n",
    "        messages = state[\"messages\"]\n",
    "        query = messages[-1].content if messages else \"\"\n",
    "        \n",
    "        if \" and \" in query.lower() or \" also \" in query.lower():\n",
    "            return \"multi_step\"\n",
    "        if messages[-1].tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "\n",
    "    def route_single_intent(self, intent: str, entities: dict) -> str:\n",
    "        try:\n",
    "            if intent == \"weather\":\n",
    "                return get_weather(entities.get(\"locations\", [\"Unknown\"])[0])\n",
    "            elif intent == \"news\":\n",
    "                return get_news(\" \".join(entities.get(\"keywords\", [])))\n",
    "            elif intent == \"finance\":\n",
    "                return get_financial_data(entities.get(\"companies\", [\"Unknown\"])[0])\n",
    "            elif intent == \"translation\":\n",
    "                return translate_text(\" \".join(entities.get(\"keywords\", [])))\n",
    "            elif intent == \"sentiment\":\n",
    "                return analyze_sentiment(\" \".join(entities.get(\"keywords\", [])))\n",
    "            return \"⚠️ Could not process request\"\n",
    "        except Exception as e:\n",
    "            return f\"⚠️ Processing error: {str(e)}\"\n",
    "\n",
    "# Semantic caching\n",
    "def check_history(query: str, threshold: float = 0.85) -> str:\n",
    "    query_embed = encoder.encode(query)\n",
    "    for i, msg in enumerate(memory.chat_memory.messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            msg_embed = encoder.encode(msg.content)\n",
    "            similarity = cosine_similarity([query_embed], [msg_embed])[0][0]\n",
    "            if similarity > threshold and (i+1) < len(memory.chat_memory.messages):\n",
    "                return memory.chat_memory.messages[i+1].content\n",
    "    return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    agent = FinancialAgent()\n",
    "    metrics = EvaluationMetrics()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "            query = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if query.lower() in ['exit', 'quit']:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                continue\n",
    "\n",
    "            # Check cached responses\n",
    "            cached = check_history(query)\n",
    "            if cached:\n",
    "                print(f\"\\nAssistant (cached): {cached}\")\n",
    "                metrics.track_query(True, time.time()-start_time)\n",
    "                continue\n",
    "\n",
    "            # Process new query\n",
    "            memory.chat_memory.add_user_message(query)\n",
    "            \n",
    "            try:\n",
    "                response = agent.workflow.invoke({\n",
    "                    \"messages\": [\n",
    "                        (msg.type, msg.content) \n",
    "                        for msg in memory.chat_memory.messages\n",
    "                    ]\n",
    "                })\n",
    "                ai_response = response[\"messages\"][-1].content\n",
    "                success = \"⚠️\" not in ai_response\n",
    "                \n",
    "                # Track tool usage\n",
    "                for tool in tools:\n",
    "                    if tool.name in ai_response:\n",
    "                        metrics.track_tool_usage(tool.name)\n",
    "                \n",
    "            except Exception as e:\n",
    "                ai_response = f\"⚠️ System error: {str(e)}\"\n",
    "                success = False\n",
    "                metrics.track_error()\n",
    "\n",
    "            # Update metrics and memory\n",
    "            metrics.track_query(success, time.time()-start_time)\n",
    "            memory.chat_memory.add_ai_message(ai_response)\n",
    "            print(f\"\\nAssistant: {ai_response}\")\n",
    "\n",
    "    finally:\n",
    "        # Save conversation history\n",
    "        try:\n",
    "            with open(memory_file, \"w\") as f:\n",
    "                history = [{\n",
    "                    \"type\": \"human\" if isinstance(msg, HumanMessage) else \"ai\",\n",
    "                    \"content\": msg.content\n",
    "                } for msg in memory.chat_memory.messages]\n",
    "                json.dump(history, f, indent=2)\n",
    "            \n",
    "            # Generate final report\n",
    "            report = metrics.generate_report()\n",
    "            print(\"\\n=== Evaluation Report ===\")\n",
    "            print(f\"Success Rate: {report['success_rate']:.1%}\")\n",
    "            print(f\"Avg Response Time: {report['avg_response_time']:.2f}s\")\n",
    "            print(\"Tool Usage:\")\n",
    "            for tool, count in report['tool_usage'].items():\n",
    "                print(f\"  - {tool}: {count}\")\n",
    "            print(f\"Error Rate: {report['error_rate']:.1%}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to save data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # Initialize the ChatGroq model\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.3-70b-versatile\",\n",
    "#     temperature=0.5,\n",
    "#     api_key=groq_api_key\n",
    "# )\n",
    "# llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
