{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shrawan-gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Import the keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "WEATHER_API_KEY = os.environ['WEATHER_API_KEY']\n",
    "TAVILY_API_KEY = os.environ['TAVILY_API_KEY']\n",
    "\n",
    "\n",
    "# Retrieve credentials from environment variables\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"ENDPOINT_URL_MINI\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_MINI\")\n",
    "deployment_name=\"shrawan-gpt-4o-mini\"\n",
    "\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "print(deployment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries and methods\n",
    "import requests\n",
    "from typing import List, Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # Initialize the ChatGroq model\n",
    "# llm = ChatGroq(\n",
    "#     model=\"llama-3.3-70b-versatile\",\n",
    "#     temperature=0.5,\n",
    "#     api_key=groq_api_key\n",
    "# )\n",
    "# llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have OpenAI key\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"sk-U7tijaa4jwHvhVWGr....\", temperature=0)\n",
    "\n",
    "\n",
    "## if you have azure openai\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_api_key\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-127ef8ed-86ce-4b30-a5d8-3a7db147b1aa-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs to Simulate\n",
    "You are required to simulate at least 3 out of the 5 following APIs using functions. The agent\n",
    "should dynamically determine the right function(s) to call based on the user&#39;s query and\n",
    "retain context for follow-up questions.\n",
    "\n",
    "#### Financial Data Function\n",
    "\n",
    "Function Name: `get_financial_data(query)`\n",
    "Example Queries:\n",
    "- What is the stock price of Tesla?\n",
    "- How is Bitcoin performing today?\n",
    "\n",
    "#### Weather Function\n",
    "Function Name: `get_weather(query)`\n",
    "Example Queries:\n",
    "- What&#39;s the weather like in Tokyo?\n",
    "- Will it rain in New York tomorrow?\n",
    "\n",
    "\n",
    "#### News Function\n",
    "Function Name: `get_news(query)`\n",
    "Example Queries:\n",
    "- Tell me the latest news about AI.\n",
    "- Any updates on the electric vehicle industry?\n",
    "\n",
    "\n",
    "#### Sentiment Analysis Function\n",
    "Function Name: `analyze_sentiment(query)`\n",
    "Example Queries:\n",
    "\n",
    "- Analyze the sentiment of &#39;The product is amazing, but delivery was slow.&#39;\n",
    "- What do people think about OpenAI?\n",
    "\n",
    "\n",
    "#### Translation Function\n",
    "Function Name: `translate_text(query)`\n",
    "Example Queries:\n",
    "- Translate &#39;How are you?&#39; into French.\n",
    "- How do you say &#39;Good morning&#39; in Spanish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(  \n",
    "    azure_endpoint=azure_openai_endpoint,  \n",
    "    api_key=azure_openai_api_key,  \n",
    "    api_version=\"2024-05-01-preview\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(client, model, query):\n",
    "    prompt='''your are an efficient translator, who first analyze the query and its language and then translate it to the target language.\n",
    "    ## given Query: {query}\n",
    "    Note : in output only provide the translation of the query, nothing else.\n",
    "    '''\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that translate the given sentence\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "        \n",
    "    )\n",
    "    return stream\n",
    "\n",
    "\n",
    "def sentiment(client, model, query):\n",
    "    prompt = f'''\n",
    "    You are a sentiment analyzer. First, analyze the sentiment of the given query and detect its language. Then, provide the sentiment analysis result.\n",
    "    ## Given Query: {query}\n",
    "    Note: In the output, only provide the sentiment result, nothing else.\n",
    "    '''\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs sentiment analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "        \n",
    "    )\n",
    "    return stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather(query: str) -> list:\n",
    "    \"\"\"Search weatherapi to get the current weather\"\"\"\n",
    "    endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={query}\"\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"location\"):\n",
    "        return data\n",
    "    else:\n",
    "        return \"Weather Data Not Found\"\n",
    "\n",
    "@tool\n",
    "def get_news(query: str) -> list:\n",
    "    \"\"\"Search the news from web\"\"\"\n",
    "    tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4, search_depth='advanced', max_tokens=1000)\n",
    "    results = tavily_search.invoke(query)\n",
    "    return results\n",
    "\n",
    "\n",
    "@tool\n",
    "def translate_text(query: str)-> str:\n",
    "    \"\"\"\n",
    "    Translates the given query text into the target language using the deployed model.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The text to be translated.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    translated_text=translate(client, deployment_name, query)\n",
    "    translated_text=translated_text.choices[0].message.content\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def analyze_sentiment(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on the provided query.\n",
    "    \n",
    "    This function utilizes the sentiment analysis model to evaluate the input query and returns the sentiment classification.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The text input for sentiment analysis.\n",
    "    \n",
    "    Returns:\n",
    "        str: The sentiment analysis result.\n",
    "    \"\"\"\n",
    "    sentiment_analyzer = sentiment(client, deployment_name, query)\n",
    "    sentiment_analyzer = sentiment_analyzer.choices[0].message.content\n",
    "    return sentiment_analyzer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather, get_weather, translate_text, analyze_sentiment]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tools(tags=None, recurse=True, func_accepts_config=True, func_accepts={'writer': False, 'store': True}, tools_by_name={'get_weather': StructuredTool(name='get_weather', description='Search weatherapi to get the current weather', args_schema=<class 'langchain_core.utils.pydantic.get_weather'>, func=<function get_weather at 0x10ed64720>), 'get_news': StructuredTool(name='get_news', description='Search the news from web', args_schema=<class 'langchain_core.utils.pydantic.get_news'>, func=<function get_news at 0x10ed647c0>), 'translate_text': StructuredTool(name='translate_text', description='Translates the given query text into the target language using the deployed model.\\n\\nArgs:\\n    query (str): The text to be translated.\\n\\nReturns:\\n    str: The translated text.', args_schema=<class 'langchain_core.utils.pydantic.translate_text'>, func=<function translate_text at 0x10ed64860>), 'analyze_sentiment': StructuredTool(name='analyze_sentiment', description='Performs sentiment analysis on the provided query.\\n\\nThis function utilizes the sentiment analysis model to evaluate the input query and returns the sentiment classification.\\n\\nArgs:\\n    query (str): The text input for sentiment analysis.\\n\\nReturns:\\n    str: The sentiment analysis result.', args_schema=<class 'langchain_core.utils.pydantic.analyze_sentiment'>, func=<function analyze_sentiment at 0x10ed65620>)}, tool_to_state_args={'get_weather': {}, 'get_news': {}, 'translate_text': {}, 'analyze_sentiment': {}}, tool_to_store_arg={'get_weather': None, 'get_news': None, 'translate_text': None, 'analyze_sentiment': None}, handle_tool_errors=True, messages_key='messages')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [get_weather, get_news, translate_text, analyze_sentiment]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "tool_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"----------------------------------call_model----------------------------------------\")\n",
    "    print(messages)\n",
    "    print(\"----------------------------------call_model----------------------------------------\")\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    print(\"---------------------------------------------\")\n",
    "    return {\"messages\": [response]}\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"----------------------------------call_tools----------------------------------------\")\n",
    "    print(messages)\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    check_tool=last_message.tool_calls\n",
    "    print(\"called tool info:\", check_tool)\n",
    "    print(\"----------------------------------call_tools----------------------------------------\")\n",
    "    \n",
    "    if check_tool:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the workflow from StateGraph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# add a node named LLM, with call_model function. This node uses an LLM to make decisions based on the input given\n",
    "workflow.add_node(\"LLM\", call_model)\n",
    "\n",
    "# Our workflow starts with the LLM node\n",
    "workflow.add_edge(START, \"LLM\")\n",
    "\n",
    "# Add a tools node\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add a conditional edge from LLM to call_tools function. It can go tools node or end depending on the output of the LLM. \n",
    "workflow.add_conditional_edges(\"LLM\", call_tools)\n",
    "\n",
    "# tools node sends the information back to the LLM\n",
    "workflow.add_edge(\"tools\", \"LLM\")\n",
    "\n",
    "agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I hate you\n",
      "----------------------------------call_model----------------------------------------\n",
      "[HumanMessage(content='I hate you', additional_kwargs={}, response_metadata={}, id='d768a265-ba98-4235-b2ac-3d9c7e9f8d60')]\n",
      "----------------------------------call_model----------------------------------------\n",
      "---------------------------------------------\n",
      "----------------------------------call_tools----------------------------------------\n",
      "[HumanMessage(content='I hate you', additional_kwargs={}, response_metadata={}, id='d768a265-ba98-4235-b2ac-3d9c7e9f8d60'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'function': {'arguments': '{\"query\":\"I hate you\"}', 'name': 'analyze_sentiment'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 184, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-5fbc3006-f48f-4722-ac12-b1df30aba84a-0', tool_calls=[{'name': 'analyze_sentiment', 'args': {'query': 'I hate you'}, 'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 18, 'total_tokens': 202, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'function': {'arguments': '{\"query\":\"I hate you\"}', 'name': 'analyze_sentiment'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 184, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}} id='run-5fbc3006-f48f-4722-ac12-b1df30aba84a-0' tool_calls=[{'name': 'analyze_sentiment', 'args': {'query': 'I hate you'}, 'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'type': 'tool_call'}] usage_metadata={'input_tokens': 184, 'output_tokens': 18, 'total_tokens': 202, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "called tool info: [{'name': 'analyze_sentiment', 'args': {'query': 'I hate you'}, 'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'type': 'tool_call'}]\n",
      "----------------------------------call_tools----------------------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  analyze_sentiment (call_sqdC6mNDDAM6Ce4W94j8W0DT)\n",
      " Call ID: call_sqdC6mNDDAM6Ce4W94j8W0DT\n",
      "  Args:\n",
      "    query: I hate you\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: analyze_sentiment\n",
      "\n",
      "Negative sentiment\n",
      "----------------------------------call_model----------------------------------------\n",
      "[HumanMessage(content='I hate you', additional_kwargs={}, response_metadata={}, id='d768a265-ba98-4235-b2ac-3d9c7e9f8d60'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'function': {'arguments': '{\"query\":\"I hate you\"}', 'name': 'analyze_sentiment'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 184, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-5fbc3006-f48f-4722-ac12-b1df30aba84a-0', tool_calls=[{'name': 'analyze_sentiment', 'args': {'query': 'I hate you'}, 'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 18, 'total_tokens': 202, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Negative sentiment', name='analyze_sentiment', id='18c5c75f-93eb-4137-8c59-56b264974119', tool_call_id='call_sqdC6mNDDAM6Ce4W94j8W0DT')]\n",
      "----------------------------------call_model----------------------------------------\n",
      "---------------------------------------------\n",
      "----------------------------------call_tools----------------------------------------\n",
      "[HumanMessage(content='I hate you', additional_kwargs={}, response_metadata={}, id='d768a265-ba98-4235-b2ac-3d9c7e9f8d60'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'function': {'arguments': '{\"query\":\"I hate you\"}', 'name': 'analyze_sentiment'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 184, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-5fbc3006-f48f-4722-ac12-b1df30aba84a-0', tool_calls=[{'name': 'analyze_sentiment', 'args': {'query': 'I hate you'}, 'id': 'call_sqdC6mNDDAM6Ce4W94j8W0DT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 184, 'output_tokens': 18, 'total_tokens': 202, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Negative sentiment', name='analyze_sentiment', id='18c5c75f-93eb-4137-8c59-56b264974119', tool_call_id='call_sqdC6mNDDAM6Ce4W94j8W0DT'), AIMessage(content=\"The sentiment expressed in your message is negative. If there's anything specific you'd like to talk about or if you need assistance, I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 214, 'total_tokens': 244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-ef50fb45-30f3-428c-a18e-7bc149deb288-0', usage_metadata={'input_tokens': 214, 'output_tokens': 30, 'total_tokens': 244, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "content=\"The sentiment expressed in your message is negative. If there's anything specific you'd like to talk about or if you need assistance, I'm here to help!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 214, 'total_tokens': 244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f3927aa00d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-ef50fb45-30f3-428c-a18e-7bc149deb288-0' usage_metadata={'input_tokens': 214, 'output_tokens': 30, 'total_tokens': 244, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "called tool info: []\n",
      "----------------------------------call_tools----------------------------------------\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The sentiment expressed in your message is negative. If there's anything specific you'd like to talk about or if you need assistance, I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"I hate you\")]},\n",
    "    stream_mode=\"values\",):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's the weather in Tokyo?\n",
      "Response: Weather in Tokyo: 72°F, Sunny\n",
      "\n",
      "Query: Give me news about artificial intelligence\n",
      "Response: Latest news about artificial intelligence: AI breakthroughs announced!\n",
      "\n",
      "Query: Translate 'Good morning' to Spanish\n",
      "Response: Translated to Spanish: Good morning\n",
      "\n",
      "Query: Tell me something random\n",
      "Response: No response generated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Optional\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define state structure\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    intent: Optional[str]\n",
    "    entities: Optional[dict]\n",
    "    result: Optional[str]\n",
    "\n",
    "\n",
    "\n",
    "# Mock services (same as before)\n",
    "def get_weather(location):\n",
    "    return f\"Weather in {location}: 72°F, Sunny\"\n",
    "\n",
    "def get_news(topic):\n",
    "    return f\"Latest news about {topic}: AI breakthroughs announced!\"\n",
    "\n",
    "def translate_text(text, target_lang):\n",
    "    return f\"Translated to {target_lang}: {text}\"\n",
    "\n",
    "# Nodes implementation\n",
    "def intent_parser(state: AgentState) -> dict:\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this query and respond in JSON format:\n",
    "    {query}\n",
    "\n",
    "    Response format:\n",
    "    {{\n",
    "        \"intent\": \"weather|news|translation\",\n",
    "        \"entities\": {{\n",
    "            \"location\": \"...\",\n",
    "            \"topic\": \"...\",\n",
    "            \"text\": \"...\",\n",
    "            \"target_lang\": \"...\"\n",
    "        }}\n",
    "    }}\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    try:\n",
    "        parsed = json.loads(re.search(r'\\{.*\\}', response, re.DOTALL).group())\n",
    "        return {\n",
    "            \"intent\": parsed[\"intent\"],\n",
    "            \"entities\": parsed[\"entities\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\"result\": \"Could not understand request\"}\n",
    "\n",
    "def weather_node(state: AgentState) -> dict:\n",
    "    location = state[\"entities\"].get(\"location\", \"unknown location\")\n",
    "    return {\"result\": get_weather(location)}\n",
    "\n",
    "def news_node(state: AgentState) -> dict:\n",
    "    topic = state[\"entities\"].get(\"topic\", \"general\")\n",
    "    return {\"result\": get_news(topic)}\n",
    "\n",
    "def translation_node(state: AgentState) -> dict:\n",
    "    entities = state[\"entities\"]\n",
    "    text = entities.get(\"text\", \"\")\n",
    "    target_lang = entities.get(\"target_lang\", \"French\")\n",
    "    return {\"result\": translate_text(text, target_lang)}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"intent_parser\", intent_parser)\n",
    "workflow.add_node(\"weather\", weather_node)\n",
    "workflow.add_node(\"news\", news_node)\n",
    "workflow.add_node(\"translation\", translation_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"intent_parser\")\n",
    "\n",
    "# Add conditional edges\n",
    "def route_based_on_intent(state: AgentState):\n",
    "    intent = state.get(\"intent\")\n",
    "    if intent == \"weather\":\n",
    "        return \"weather\"\n",
    "    elif intent == \"news\":\n",
    "        return \"news\"\n",
    "    elif intent == \"translation\":\n",
    "        return \"translation\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"intent_parser\",\n",
    "    route_based_on_intent,\n",
    "    {\n",
    "        \"weather\": \"weather\",\n",
    "        \"news\": \"news\",\n",
    "        \"translation\": \"translation\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges for service nodes\n",
    "workflow.add_edge(\"weather\", END)\n",
    "workflow.add_edge(\"news\", END)\n",
    "workflow.add_edge(\"translation\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "# Run the agent\n",
    "def run_agent(query):\n",
    "    result = agent.invoke({\"query\": query})\n",
    "    return result.get(\"result\", \"No response generated\")\n",
    "\n",
    "# Example usage (same as before)\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"What's the weather in Tokyo?\",\n",
    "        \"Give me news about artificial intelligence\",\n",
    "        \"Translate 'Good morning' to Spanish\",\n",
    "        \"Tell me something random\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Response: {run_agent(query)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's the weather in Tokyo?\n",
      "Response: Weather in Tokyo: 72°F, Sunny\n",
      "\n",
      "Query: Give me news about artificial intelligence\n",
      "Response: Latest news about artificial intelligence: AI breakthroughs announced!\n",
      "\n",
      "Query: Translate 'Good morning' to Spanish\n",
      "Response: \"Buenos días\"\n",
      "\n",
      "Query: Analyze the sentiment of 'I am very happy today!'\n",
      "Response: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Optional\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the state structure\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    intent: Optional[str]\n",
    "    entities: Optional[dict]\n",
    "    result: Optional[str]\n",
    "\n",
    "# Initialize the language model client\n",
    "# llm = ChatOpenAI(api_key='your_openai_api_key')  # Replace with your actual API key\n",
    "\n",
    "# Define the intent parser node\n",
    "def intent_parser(state: AgentState) -> dict:\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this query and respond in JSON format:\n",
    "    {query}\n",
    "\n",
    "    Response format:\n",
    "    {{\n",
    "        \"intent\": \"weather|news|translation|sentiment\",\n",
    "        \"entities\": {{\n",
    "            \"location\": \"...\",\n",
    "            \"topic\": \"...\",\n",
    "            \"text\": \"...\",\n",
    "            \"target_lang\": \"...\"\n",
    "        }}\n",
    "    }}\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    try:\n",
    "        parsed = json.loads(re.search(r'\\{.*\\}', response, re.DOTALL).group())\n",
    "        return {\n",
    "            \"intent\": parsed[\"intent\"],\n",
    "            \"entities\": parsed[\"entities\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\"result\": \"Could not understand the request\"}\n",
    "\n",
    "# Define the weather node\n",
    "def weather_node(state: AgentState) -> dict:\n",
    "    location = state[\"entities\"].get(\"location\", \"unknown location\")\n",
    "    return {\"result\": f\"Weather in {location}: 72°F, Sunny\"}  # Mocked response\n",
    "\n",
    "# Define the news node\n",
    "def news_node(state: AgentState) -> dict:\n",
    "    topic = state[\"entities\"].get(\"topic\", \"general\")\n",
    "    return {\"result\": f\"Latest news about {topic}: AI breakthroughs announced!\"}  # Mocked response\n",
    "\n",
    "# Define the translation node\n",
    "def translation_node(state: AgentState) -> dict:\n",
    "    entities = state[\"entities\"]\n",
    "    text = entities.get(\"text\", \"\")\n",
    "    target_lang = entities.get(\"target_lang\", \"French\")\n",
    "    prompt = f'''You are an efficient translator. Translate the following text to {target_lang}:\n",
    "    \"{text}\"\n",
    "    Note: Provide only the translated text.'''\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates the given sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages).content\n",
    "    return {\"result\": response.strip()}\n",
    "\n",
    "# Define the sentiment analysis node\n",
    "def sentiment_node(state: AgentState) -> dict:\n",
    "    text = state[\"entities\"].get(\"text\", \"\")\n",
    "    prompt = f'''You are a sentiment analyzer. Analyze the sentiment of the following text:\n",
    "    \"{text}\"\n",
    "    Note: Provide only the sentiment result (e.g., Positive, Negative, Neutral).'''\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs sentiment analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages).content\n",
    "    return {\"result\": response.strip()}\n",
    "\n",
    "# Create the workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"intent_parser\", intent_parser)\n",
    "workflow.add_node(\"weather\", weather_node)\n",
    "workflow.add_node(\"news\", news_node)\n",
    "workflow.add_node(\"translation\", translation_node)\n",
    "workflow.add_node(\"sentiment\", sentiment_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"intent_parser\")\n",
    "\n",
    "# Define the routing function based on intent\n",
    "def route_based_on_intent(state: AgentState):\n",
    "    intent = state.get(\"intent\")\n",
    "    if intent == \"weather\":\n",
    "        return \"weather\"\n",
    "    elif intent == \"news\":\n",
    "        return \"news\"\n",
    "    elif intent == \"translation\":\n",
    "        return \"translation\"\n",
    "    elif intent == \"sentiment\":\n",
    "        return \"sentiment\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"intent_parser\",\n",
    "    route_based_on_intent,\n",
    "    {\n",
    "        \"weather\": \"weather\",\n",
    "        \"news\": \"news\",\n",
    "        \"translation\": \"translation\",\n",
    "        \"sentiment\": \"sentiment\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges for service nodes\n",
    "workflow.add_edge(\"weather\", END)\n",
    "workflow.add_edge(\"news\", END)\n",
    "workflow.add_edge(\"translation\", END)\n",
    "workflow.add_edge(\"sentiment\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "# Function to run the agent\n",
    "def run_agent(query):\n",
    "    result = agent.invoke({\"query\": query})\n",
    "    return result.get(\"result\", \"No response generated\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"What's the weather in Tokyo?\",\n",
    "        \"Give me news about artificial intelligence\",\n",
    "        \"Translate 'Good morning' to Spanish\",\n",
    "        \"Analyze the sentiment of 'I am very happy today!'\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Response: {run_agent(query)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's the weather in Tokyo?\n",
      "Response: Weather in Tokyo: 9.3°C, Partly cloudy\n",
      "\n",
      "Query: Give me news about artificial intelligence\n",
      "Response: Latest news about artificial intelligence:\n",
      "- No title available: No snippet available\n",
      "- No title available: No snippet available\n",
      "- No title available: No snippet available\n",
      "- No title available: No snippet available\n",
      "\n",
      "Query: Translate 'Good morning' to Spanish\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AzureChatOpenAI' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 208\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrun_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 194\u001b[0m, in \u001b[0;36mrun_agent\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_agent\u001b[39m(query):\n\u001b[0;32m--> 194\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/langgraph/pregel/__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1936\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/langgraph/pregel/__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1656\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/langgraph/pregel/runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/langgraph/utils/runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[23], line 135\u001b[0m, in \u001b[0;36mtranslation_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    133\u001b[0m text \u001b[38;5;241m=\u001b[39m entities\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m target_lang \u001b[38;5;241m=\u001b[39m entities\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_lang\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrench\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myour_model_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m: translated_text}\n",
      "Cell \u001b[0;32mIn[23], line 64\u001b[0m, in \u001b[0;36mtranslate_text\u001b[0;34m(client, model, text, target_lang)\u001b[0m\n\u001b[1;32m     55\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mYou are an efficient translator. Translate the following text to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m## Given Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124mNote: In the output, only provide the translation of the text, nothing else.\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     59\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that translates the given sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     61\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[1;32m     62\u001b[0m ]\n\u001b[0;32m---> 64\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     65\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     66\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m     67\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     68\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Desktop/MY DS Code/Langgraph/env/lib/python3.13/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AzureChatOpenAI' object has no attribute 'chat'",
      "\u001b[0mDuring task with name 'translation' and id '544bc9db-e762-b289-22d0-23ab57ac587f'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict, Optional\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define state structure\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    intent: Optional[str]\n",
    "    entities: Optional[dict]\n",
    "    result: Optional[str]\n",
    "\n",
    "# Function to get weather information\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Fetches the current weather for a given location using WeatherAPI.\"\"\"\n",
    "    endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={location}\"\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"location\"):\n",
    "        location_name = data[\"location\"][\"name\"]\n",
    "        temp_c = data[\"current\"][\"temp_c\"]\n",
    "        condition = data[\"current\"][\"condition\"][\"text\"]\n",
    "        return f\"Weather in {location_name}: {temp_c}°C, {condition}\"\n",
    "    else:\n",
    "        return \"Weather data not found.\"\n",
    "\n",
    "def get_news(topic: str) -> str:\n",
    "    \"\"\"Fetches the latest news about a given topic using Tavily Search.\"\"\"\n",
    "    tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=4, search_depth='advanced', max_tokens=1000)\n",
    "    results = tavily_search.invoke({\"query\": topic})\n",
    "    \n",
    "    if results:\n",
    "        news_items = []\n",
    "        for item in results:\n",
    "            title = item.get('title', 'No title available')\n",
    "            snippet = item.get('snippet', 'No snippet available')\n",
    "            news_items.append(f\"- {title}: {snippet}\")\n",
    "        \n",
    "        return f\"Latest news about {topic}:\\n\" + \"\\n\".join(news_items)\n",
    "    else:\n",
    "        return \"No news found on this topic.\"\n",
    "\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(client, model, text: str, target_lang: str) -> str:\n",
    "    \"\"\"Translates the given text into the target language using the provided model.\"\"\"\n",
    "    prompt = f'''You are an efficient translator. Translate the following text to {target_lang}:\n",
    "    ## Given Text: {text}\n",
    "    Note: In the output, only provide the translation of the text, nothing else.'''\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates the given sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(client, model, text: str) -> str:\n",
    "    \"\"\"Performs sentiment analysis on the provided text using the provided model.\"\"\"\n",
    "    prompt = f'''You are a sentiment analyzer. Analyze the sentiment of the following text and detect its language. Provide the sentiment analysis result.\n",
    "    ## Given Text: {text}\n",
    "    Note: In the output, only provide the sentiment result, nothing else.'''\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs sentiment analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Node to parse intent and entities\n",
    "def intent_parser(state: AgentState) -> dict:\n",
    "    query = state[\"query\"]\n",
    "    \n",
    "    prompt = f'''Analyze this query and respond in JSON format:\n",
    "    {query}\n",
    "\n",
    "    Response format:\n",
    "    {{\n",
    "        \"intent\": \"weather|news|translation|sentiment\",\n",
    "        \"entities\": {{\n",
    "            \"location\": \"...\",\n",
    "            \"topic\": \"...\",\n",
    "            \"text\": \"...\",\n",
    "            \"target_lang\": \"...\"\n",
    "        }}\n",
    "    }}'''\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)]).content\n",
    "    try:\n",
    "        parsed = json.loads(re.search(r'\\{.*\\}', response, re.DOTALL).group())\n",
    "        return {\n",
    "            \"intent\": parsed[\"intent\"],\n",
    "            \"entities\": parsed[\"entities\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\"result\": \"Could not understand the request.\"}\n",
    "\n",
    "# Node to handle weather queries\n",
    "def weather_node(state: AgentState) -> dict:\n",
    "    location = state[\"entities\"].get(\"location\", \"unknown location\")\n",
    "    return {\"result\": get_weather(location)}\n",
    "\n",
    "# Node to handle news queries\n",
    "def news_node(state: AgentState) -> dict:\n",
    "    topic = state[\"entities\"].get(\"topic\", \"general\")\n",
    "    return {\"result\": get_news(topic)}\n",
    "\n",
    "# Node to handle translation requests\n",
    "def translation_node(state: AgentState) -> dict:\n",
    "    entities = state[\"entities\"]\n",
    "    text = entities.get(\"text\", \"\")\n",
    "    target_lang = entities.get(\"target_lang\", \"French\")\n",
    "    translated_text = translate_text(llm, 'your_model_name', text, target_lang)\n",
    "    return {\"result\": translated_text}\n",
    "\n",
    "# Node to handle sentiment analysis requests\n",
    "def sentiment_node(state: AgentState) -> dict:\n",
    "    text = state[\"entities\"].get(\"text\", \"\")\n",
    "    sentiment_result = analyze_sentiment(llm, 'your_model_name', text)\n",
    "    return {\"result\": sentiment_result}\n",
    "\n",
    "# Create workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"intent_parser\", intent_parser)\n",
    "workflow.add_node(\"weather\", weather_node)\n",
    "workflow.add_node(\"news\", news_node)\n",
    "workflow.add_node(\"translation\", translation_node)\n",
    "workflow.add_node(\"sentiment\", sentiment_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"intent_parser\")\n",
    "\n",
    "# Add conditional edges\n",
    "def route_based_on_intent(state: AgentState):\n",
    "    intent = state.get(\"intent\")\n",
    "    if intent == \"weather\":\n",
    "        return \"weather\"\n",
    "    elif intent == \"news\":\n",
    "        return \"news\"\n",
    "    elif intent == \"translation\":\n",
    "        return \"translation\"\n",
    "    elif intent == \"sentiment\":\n",
    "        return \"sentiment\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"intent_parser\",\n",
    "    route_based_on_intent,\n",
    "    {\n",
    "        \"weather\": \"weather\",\n",
    "        \"news\": \"news\",\n",
    "        \"translation\": \"translation\",\n",
    "        \"sentiment\": \"sentiment\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges for service nodes\n",
    "workflow.add_edge(\"weather\", END)\n",
    "workflow.add_edge(\"news\", END)\n",
    "workflow.add_edge(\"translation\", END)\n",
    "workflow.add_edge(\"sentiment\", END)\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "# Run the agent\n",
    "def run_agent(query):\n",
    "    result = agent.invoke({\"query\": query})\n",
    "    return result.get(\"result\", \"No response generated.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"What's the weather in Tokyo?\",\n",
    "        \"Give me news about artificial intelligence\",\n",
    "        \"Translate 'Good morning' to Spanish\",\n",
    "        \"Analyze the sentiment of 'I am very happy today!'\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Response: {run_agent(query)}\\n\")\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Work-from-Home (WFH) Assignment: Intelligent API Router\n",
    "Objective\n",
    "The goal of this assignment is to develop an AI-powered agent framework that processes\n",
    "free-text queries, intelligently routes them to the appropriate function(s) based on intent,\n",
    "and utilizes memory to maintain context across interactions. The focus is on intent\n",
    "detection, routing logic, memory usage, and modular agent design rather than backend API\n",
    "development.\n",
    "APIs to Simulate\n",
    "You are required to simulate at least 3 out of the 5 following APIs using functions. The agent\n",
    "should dynamically determine the right function(s) to call based on the user&#39;s query and\n",
    "retain context for follow-up questions.\n",
    "Financial Data Function\n",
    "Function Name: `get_financial_data(query)`\n",
    "Example Queries:\n",
    "- What is the stock price of Tesla?\n",
    "- How is Bitcoin performing today?\n",
    "Weather Function\n",
    "Function Name: `get_weather(query)`\n",
    "Example Queries:\n",
    "- What&#39;s the weather like in Tokyo?\n",
    "- Will it rain in New York tomorrow?\n",
    "News Function\n",
    "Function Name: `get_news(query)`\n",
    "Example Queries:\n",
    "- Tell me the latest news about AI.\n",
    "- Any updates on the electric vehicle industry?\n",
    "Sentiment Analysis Function\n",
    "Function Name: `analyze_sentiment(query)`\n",
    "Example Queries:\n",
    "\n",
    "- Analyze the sentiment of &#39;The product is amazing, but delivery was slow.&#39;\n",
    "- What do people think about OpenAI?\n",
    "Translation Function\n",
    "Function Name: `translate_text(query)`\n",
    "Example Queries:\n",
    "- Translate &#39;How are you?&#39; into French.\n",
    "- How do you say &#39;Good morning&#39; in Spanish?\n",
    "Task: Build an AI Agent-Based Router\n",
    "Develop an AI-driven agent framework that takes free-text input, identifies intent, extracts\n",
    "entities, routes the query to the appropriate function(s), and retains memory to support\n",
    "multi-turn conversations.\n",
    "1️⃣ Input:\n",
    "Users send a free-text query to the agent framework.\n",
    "Example request:\n",
    "{\n",
    "&quot;query&quot;: &quot;What is the stock price of Tesla?&quot;\n",
    "}\n",
    "2️⃣ Processing:\n",
    "You must use an agent framework (e.g., LangChain, OpenAI Agents, CrewAI, etc.) to\n",
    "structure the routing logic.\n",
    "Your agent should:\n",
    "- Identify Intent: Determine if the query is about finance, weather, news, sentiment\n",
    "analysis, or translation.\n",
    "- Extract Key Entities: Identify relevant companies, locations, and keywords.\n",
    "- Route to Correct Function(s): Determine which function(s) should be called.\n",
    "- Handle Edge Cases: If the intent is unclear, the agent should respond accordingly.\n",
    "- Utilize Memory: Implement memory retention where the agent can remember past\n",
    "interactions and refine responses accordingly.\n",
    "3️⃣ Memory Usage &amp; Expected Behavior:\n",
    "Your agent should remember past interactions and adjust its responses accordingly.\n",
    "Example:\n",
    "\n",
    "User Query Agent Memory (Stored) Action Taken\n",
    "What is the stock price of\n",
    "Tesla?\n",
    "\n",
    "Intent: Finance (Stock) Calls Financial Data\n",
    "\n",
    "Function\n",
    "\n",
    "What about Apple? Recognizes previous stock\n",
    "\n",
    "query\n",
    "\n",
    "Calls Financial Data\n",
    "Function for Apple\n",
    "Tell me the latest news. Intent: News Calls News Function\n",
    "And Bitcoin’s\n",
    "performance?\n",
    "\n",
    "Recognizes Finance topic Calls Financial Data\n",
    "Function for Bitcoin\n",
    "\n",
    "Translate &#39;Hello&#39; into\n",
    "French.\n",
    "\n",
    "Intent: Translation Calls Translation Function\n",
    "The agent should be able to recall previous interactions and handle follow-up queries\n",
    "accordingly.\n",
    "4️⃣ Memory Implementation Options:\n",
    "You may use an existing memory solution or implement a simple dictionary-based memory.\n",
    "Suggested approaches:\n",
    "- LangChain Memory: Use `ConversationBufferMemory` or\n",
    "`ConversationSummaryMemory` to track user queries.\n",
    "- Custom Python Dictionary-Based Memory: Store previous intents in a dictionary and\n",
    "retrieve them when needed.\n",
    "Example for LangChain Memory:\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({&quot;input&quot;: &quot;What is the stock price of Tesla?&quot;}, {&quot;output&quot;: &quot;Tesla&#39;s stock\n",
    "price is $824.32.&quot;})\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "Deliverables\n",
    "You are required to submit the following:\n",
    "1. Agent Design Document (1-2 pages)\n",
    "- Explain the architecture of the agent framework.\n",
    "- How does the agent process and classify queries?\n",
    "- How does the agent map queries to functions?\n",
    "- How does the agent handle memory usage, edge cases, and multi-step queries?\n",
    "2. Python Implementation\n",
    "- Implement an agent-driven function router.\n",
    "- Implement at least 3 function endpoints (`get_financial_data`, `get_weather`, etc.).\n",
    "- Use a popular agent framework (LangChain, OpenAI Agents, CrewAI, etc.).\n",
    "\n",
    "3. Example Queries &amp; Responses\n",
    "- Provide 5 example queries with expected function outputs.\n",
    "Evaluation Criteria\n",
    "✅ Routing Logic &amp; Intent Detection – How well queries are classified and routed.\n",
    "✅ Agent Framework Implementation – How effectively an agent framework is used.\n",
    "✅ Memory Usage – How well memory is utilized for multi-turn interactions.\n",
    "✅ Code Quality – Readable, maintainable, and well-structured.\n",
    "✅ Handling Edge Cases – Graceful failure handling and proper response messages.\n",
    "✅ Bonus – If the agent can handle multi-step queries such as &#39;Stock price of Apple and\n",
    "sentiment around it.&#39; by calling multiple functions sequentially.\n",
    "Bonus Challenge (Optional)\n",
    "For advanced candidates, allow memory to persist across multiple interactions, so a user\n",
    "does not lose context even if they exit and return later.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "your task is to implemet the soution using langraph you can  take the reference for below code:\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "WEATHER_API_KEY = os.environ['WEATHER_API_KEY']\n",
    "TAVILY_API_KEY = os.environ['TAVILY_API_KEY']\n",
    "\n",
    "\n",
    "# Retrieve credentials from environment variables\n",
    "openai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "azure_openai_endpoint = os.getenv(\"ENDPOINT_URL_MINI\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY_MINI\")\n",
    "\n",
    "# Import the required libraries and methods\n",
    "import requests\n",
    "from typing import List, Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# If you have OpenAI key\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"sk-U7tijaa4jwHvhVWGr....\", temperature=0)\n",
    "\n",
    "\n",
    "## if you have azure openai\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model=  \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "if not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\n",
    "  os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_api_key\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_deployment=model,\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "@tool\n",
    "def get_weather(query: str) -> list:\n",
    "    \"\"\"Search weatherapi to get the current weather\"\"\"\n",
    "    endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={query}\"\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "\n",
    "    if data.get(\"location\"):\n",
    "        return data\n",
    "    else:\n",
    "        return \"Weather Data Not Found\"\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> list:\n",
    "    \"\"\"Search the web for a query\"\"\"\n",
    "    tavily_search = TavilySearchResults(api_key=TAVILY_API_KEY, max_results=2, search_depth='advanced', max_tokens=1000)\n",
    "    results = tavily_search.invoke(query)\n",
    "    return results\n",
    "\n",
    "tools = [search_web, get_weather]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "tools = [search_web, get_weather]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"----------------------------------call_model----------------------------------------\")\n",
    "    print(messages)\n",
    "    print(\"----------------------------------call_model----------------------------------------\")\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    print(\"----------------------------------call_tools----------------------------------------\")\n",
    "    print(messages)\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    check_tool=last_message.tool_calls\n",
    "    print(\"called tool info:\", check_tool)\n",
    "    print(\"----------------------------------call_tools----------------------------------------\")\n",
    "    \n",
    "    if check_tool:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# initialize the workflow from StateGraph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# add a node named LLM, with call_model function. This node uses an LLM to make decisions based on the input given\n",
    "workflow.add_node(\"LLM\", call_model)\n",
    "\n",
    "# Our workflow starts with the LLM node\n",
    "workflow.add_edge(START, \"LLM\")\n",
    "\n",
    "# Add a tools node\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add a conditional edge from LLM to call_tools function. It can go tools node or end depending on the output of the LLM. \n",
    "workflow.add_conditional_edges(\"LLM\", call_tools)\n",
    "\n",
    "# tools node sends the information back to the LLM\n",
    "workflow.add_edge(\"tools\", \"LLM\")\n",
    "\n",
    "agent = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [(\"user\", \"What is the stock price of Tesla?\")]},\n",
    "    stream_mode=\"values\",):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
